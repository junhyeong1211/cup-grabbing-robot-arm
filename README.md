# 컵을 스스로 찾아 잡는 로봇팔 만들기 프로젝트

## 1. 프로젝트 목표 

이 프로젝트는 시뮬레이션 환경에서 **'인지-판단-제어'로 이어지는 완전한 AI 로보틱스 파이프라인을 구현**하는 것을 목표로 한다. 

**[인지]** 가상 카메라의 이미지 스트림을 입력받아 CNN 기반의 객체 탐지 모델을 사용하여 테이블 위 **컵의 3차원 위치와 자세를 정확히 추정** 한다.
**[판단]** 인지된 컵의 위치를 목표로 Reinforcement Learning 에이전트를 통해 로봇팔의 각 관절을 제어하여 **장애물을 회피하고 최적의 경로를 생성** 한다.
**[제어]** 생성된 경로 계획을 바탕으로 PyBullet 시뮬레이터 내에서 로봇팔을 **정확하고 안정적으로 움직여 컵을 잡는 동작을 수행** 한다.

## 2. 프로젝트 동기 

"The Limits and Potentials of Deep Learning for Robotics" 논문은 현재 딥러닝이 단순한 패턴 인식을 넘어, 실제 물리 세계와 상호작용하기 위해 'Embodiment'과 'Reasoning' 능력이 필수적임을 강조한다. 이 프로젝트는 해당 논문에서 얻은 통찰을 바탕으로 이론적 학습을 넘어 실제 통합 과정의 어려움을 직접 경험하고 해결하기 위해 시작되었다.

단순히 튜토리얼 코드를 실행하는 것을 넘어서 **서로 다른 AI 모델(컴퓨터 비전, 강화학습)을 하나의 목표 아래 유기적으로 연결할 때 발생하는 실질적인 문제들을 해결**하는 데 집중하고자 한다. 이를 통해 특정 기술의 원리뿐만 아니라, 전체 시스템을 설계하고 통합하는 엔지니어링 역량을 기르는 것을 목표로 한다.

## 3. 프로젝트 단계 

1.  **프로젝트 설계 및 환경 구축:** 목표 정의, 개발 환경 및 시뮬레이터 설정
2.  **[인지] 시각 모듈 개발:** CNN 기반의 객체 탐지 모델을 활용한 **컵의 3D 공간 좌표 추정** 기능 구현 (YOLO를 이용한 2D Bounding Box 검출 및 Depth 정보를 활용한 3D 좌표 변환)
3.  **[판단] 정책 모듈 개발:** 강화학습 환경 구성 및 **추정된 3D 목표 지점을 향한** 로봇팔 제어 정책 학습
4.  **[제어] 시뮬레이션 통합:** 물리 시뮬레이터 내 로봇팔 모델링 및 제어 인터페이스 개발
5.  **시스템 통합 및 테스트:** 인지, 판단, 제어 모듈을 연결하여 **가상 3D 공간**에서 최종 목표(컵 잡기) 테스트 및 성능 개선
6.  **프로젝트 회고 및 문서화:** 결과 분석, 배운 점 정리, 향후 개선 방향 제시
