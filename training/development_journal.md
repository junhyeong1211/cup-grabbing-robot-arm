# '컵 잡기' AI 개발 전체 회고록 (A to Z)

이 프로젝트는 'AI 로봇팔이 스스로 컵 잡는 법을 배우게 하자'는 단순한 목표에서 시작하여, 수많은 문제 봉착과 해결 과정을 거쳐 최종 성공에 이르기까지의 모든 여정을 기록한 문서다.

---

## 1. 초기 모델의 실패 : '게으른 AI'의 등장

#### **목표 및 설계**
* **환경:** KUKA 로봇팔을 이용한 간단한 '컵에 도달하기(Reaching)' 환경.
* **상태(State):** 그리퍼 위치, 컵 위치.
* **행동(Action):** 그리퍼의 XYZ 이동.
* **보상:** 거리가 가까워지면 약간의 보상, 컵을 넘어뜨리면 큰 페널티(`-500`).

#### **문제 현상**
훈련 후, AI가 컵을 잡으러 가지 않고 제자리에서 최소한으로만 움직이는 '몸을 사리는' 행동을 보였습다. 훈련 로그의 평균 보상(`ep_rew_mean`)이 `-200`점대의 낮은 점수에 수렴하며 더 이상 학습이 진행되지 않았다.

#### **원인 분석**
AI는 우리가 설계한 보상 함수의 허점을 파고들었다.
> "컵에 접근하다가 실수로 넘어뜨리면 **-500점**이라는 큰 벌점을 받는다. 하지만 가만히 있으면 **매 스텝당 -1점**이라는 작은 시간 페널티만 받는다. 따라서, **아무것도 안 하는 것이 가장 손해를 적게 보는 최적의 전략이다!**"

#### **주요 깨달음**
AI는 나의 '의도'가 아닌, '코드'로 구현된 보상 규칙을 맹목적으로 따른다는 첫 번째 교훈을 얻었다.

---

## 2. '대담한 AI'를 위한 환경 개선 (

#### **목표 및 가설**
'게으른 AI' 문제를 해결하기 위해서 AI가 실패를 두려워하지 않고 더 과감하게 탐험하도록 만들어야 한다는 가설을 세웠다.

#### **적용된 수정 사항**
1.  **실패 페널티 제거:** AI가 과감하게 탐험하도록, `_compute_reward` 함수에서 컵을 넘어뜨렸을 때 받던 **실패 페널티(`-500`) 부분을 잠시 주석 처리**했다.
2.  **`gamma` 값 조정:** AI가 미래의 큰 성공 보상을 더 가치 있게 여기도록 PPO 모델의 `gamma` 값을 `0.99`로 높였다.
    ```python
    model = PPO("MlpPolicy", env, verbose=1, gamma=0.99)
    ```

#### **결과 및 새로운 문제**
AI는 이전보다 훨씬 적극적으로 컵을 향해 움직이기 시작했다. 평균 보상(`ep_rew_mean`)도 마이너스 폭이 줄어들며 학습이 진행되는 듯 보였다. 하지만, 결코 '성공'에는 도달하지 못하고 '시간 초과'로 에피소드를 끝내는 패턴을 반복했다. AI는 이제 **'겁은 없어졌지만, 여전히 길을 모르는'** 상태였다.

---

## 3. '진짜 잡기'를 위한 대대적인 업그레이드 

'단순 접근'을 넘어서 '성공적인 잡기'를 위해서는 환경 자체의 근본적인 업그레이드가 필요하다고 판단했다.

#### **적용된 수정 사항**

1.  **로봇 모델 교체 (KUKA → Franka Panda):**
    * **문제:** 기존 `kuka_iiwa/model.urdf` 모델에는 제어 가능한 그리퍼(손)가 없다는 치명적인 문제를 발견했다.
    * **해결:** 그리퍼가 기본적으로 포함된 `franka_panda/panda.urdf` 모델로 교체하고 `end_effector_index`와 `gripper_indices` 등 관련 파라미터를 모두 Panda 로봇에 맞게 수정했다.

2.  **Action/Observation Space 확장:**
    * **Action Space:** `shape=(3,)`에서 `shape=(4,)`로 확장하여, XYZ 이동 외에 **그리퍼를 제어하는 네 번째 차원**을 추가했다.
    * **Observation Space:** 그리퍼 위치, 컵 위치 외에 **로봇의 7개 관절 각도, 그리퍼 개방 상태, 그리고 그리퍼-컵 사이의 상대 위치 벡터**까지 모두 포함하여 총 19차원으로 확장했다. 이를 통해 AI는 자신의 몸 상태와 목표와의 관계를 훨씬 더 풍부하게 인지할 수 있게 되었다.

3.  **'커리큘럼 학습' 도입 (난이도 조절):**
    * **문제:** 처음부터 무작위 위치의 컵을 잡는 것은 AI에게 너무 어려운 문제일 수 있다.
    * **해결:** `reset` 함수에서 컵의 시작 위치를 랜덤이 아닌 **고정된 값(`[0.5, 0.2, 0.05]`)**으로 변경했다. AI가 먼저 '쉬운 문제'부터 확실하게 풀도록 유도하기 위함이다.

4.  **물리 파라미터 튜닝 (마찰력):**
    * **문제:** 그리퍼가 컵을 잡아도 물리 시뮬레이션 상에서 미끄러져 놓칠 수 있다.
    * **해결:** `p.changeDynamics` 함수를 이용해 컵과 그리퍼의 **마찰력(`lateralFriction`)을 높여서** 잡기 성공률을 현실적으로 개선했다.

5.  **보상 함수 대폭 개선 ('보상의 계단'):**
    * AI가 막막한 최종 목표까지 쉽게 도달할 수 있도록, 다음과 같이 여러 단계의 '보상 계단'을 설계했다.
        1.  **접근 보상:** `(이전 거리 - 현재 거리)`에 비례한 보상.
        2.  **접촉 보상:** 컵에 닿기만 해도 `+100`점 보너스.
        3.  **잡기 시도 보상:** 컵 근처에서 그리퍼를 닫으면 `+200`점 추가 보너스.
        4.  **최종 성공 보상:** 컵을 들어 올리면 `+5000`~`+10000`점 '잭팟'.

6.  **훈련 하이퍼파라미터 튜닝:**
    * 더 안정적이고 깊이 있는 학습을 위해 PPO 모델의 하이퍼파라미터를 조정했다.
    ```python
    model = PPO(
        "MlpPolicy", env, verbose=1,
        gamma=0.99,            # 미래 가치 중시
        learning_rate=0.0001,  # 안정적 학습
        n_steps=4096           # 풍부한 데이터 기반 업데이트
    )
    ```

---

## 4. 최종 성공 및 결론

위와 같은 복합적인 개선 끝에 AI는 마침내 훈련 과정에서 **'성공! 컵을 들어 올렸습니다!'** 메시지를 출력하며 최종 목표를 달성하는 데 성공했다.

이번 프로젝트를 통해 강화학습의 성공은 단순히 좋은 알고리즘을 사용하는 것을 넘어서 **AI의 눈높이에 맞춰 문제를 정의하고(State/Action), 명확한 이정표를 제시하며(Reward Shaping), 점진적으로 난이도를 높여가는(Curriculum Learning) 체계적인 '교육 설계'가 핵심**임을 깨달았다.
