# '컵 잡기' AI 개발 전체 회고록 

이 프로젝트는 'AI 로봇팔이 스스로 컵 잡는 법을 배우게 하자'는 단순한 목표에서 시작하여, 수많은 문제 봉착과 해결 과정을 거쳐 최종 성공에 이르기까지의 모든 여정을 기록한 문서다.

### **주요 기술 스택**

* **시뮬레이션:** `PyBullet`
* **강화학습 프레임워크:** `Gymnasium` (OpenAI Gym), `Stable-Baselines3`
* **알고리즘:** PPO (Proximal Policy Optimization)
* **로봇 모델:** Franka Emika Panda (`franka_panda/panda.urdf`)

## 1. 초기 모델의 실패 : '게으른 AI'

#### **목표 및 설계**
* **환경:** KUKA 로봇팔을 이용한 간단한 '컵에 도달하기' 환경.
* **상태(State):** 그리퍼 위치, 컵 위치.
* **행동(Action):** 그리퍼의 XYZ 이동.
* **보상:** 거리가 가까워지면 약간의 보상, 컵을 넘어뜨리면 큰 페널티(`-500`), 시간 페널티(`-1`).

#### **문제 현상**
훈련 후, AI가 컵을 잡으러 가지 않고 제자리에서 최소한으로만 움직이는 '몸을 사리는' 행동을 보였다. 훈련 로그의 평균 보상(`ep_rew_mean`)이 `-200`점대의 낮은 점수에 수렴하며 더 이상 학습이 진행되지 않았다.

#### **원인 분석**
AI는 우리가 설계한 보상 함수의 허점을 파고들었다.
> "컵에 접근하다가 실수로 넘어뜨리면 **-500점**이라는 큰 벌점을 받는다. 하지만 가만히 있으면 **매 스텝당 -1점**이라는 작은 시간 페널티만 받는다. 따라서, **아무것도 안 하는 것이 가장 손해를 적게 보는 최적의 전략이다!**"

#### **주요 깨달음**
AI는 나의 '의도'가 아닌, '코드'로 구현된 보상 규칙을 맹목적으로 따른다는 첫 번째 교훈을 얻었다.

---

## 2. '대담한 AI'를 위한 환경 개선

#### **목표 및 가설**
'게으른 AI' 문제를 해결하기 위해서 AI가 실패를 두려워하지 않고 더 과감하게 탐험하도록 만들어야 한다는 가설을 세웠다.

#### **적용된 수정 사항**
1.  **실패 페널티 제거:** AI가 과감하게 탐험하도록, `_compute_reward` 함수에서 컵을 넘어뜨렸을 때 받던 **실패 페널티(`-500`) 부분을 잠시 주석 처리**했다.
2.  **`gamma` 값 조정:** AI가 미래의 큰 성공 보상을 더 가치 있게 여기도록 PPO 모델의 `gamma` 값을 `0.99`로 높였다.
    ```python
    model = PPO("MlpPolicy", env, verbose=1, gamma=0.99)
    ```

#### **결과 및 새로운 문제**
AI는 이전보다 훨씬 적극적으로 컵을 향해 움직이기 시작했다. 평균 보상(`ep_rew_mean`)도 마이너스 폭이 줄어들며 학습이 진행되는 듯 보였다. 하지만, 결코 '성공'에는 도달하지 못하고 '시간 초과'로 에피소드를 끝내는 패턴을 반복했다. AI는 이제 **'겁은 없어졌지만, 여전히 길을 모르는'** 상태였다.

---

## 3. '진짜 잡기'를 위한 대대적인 업그레이드 

'단순 접근'을 넘어서 '성공적인 잡기'를 위해서는 환경 자체의 근본적인 업그레이드가 필요하다고 판단했다.

#### **적용된 수정 사항**

1.  **로봇 모델 교체 (KUKA → Franka Panda):**
    * **문제:** 기존 `kuka_iiwa/model.urdf` 모델에는 제어 가능한 그리퍼(손)가 없다는 치명적인 문제를 발견했다.
    * **해결:** 그리퍼가 기본적으로 포함된 `franka_panda/panda.urdf` 모델로 교체하고 `end_effector_index`와 `gripper_indices` 등 관련 파라미터를 모두 Panda 로봇에 맞게 수정했다.

2.  **Action/Observation Space 확장:**
    * **Action Space:** `shape=(3,)`에서 `shape=(4,)`로 확장하여, XYZ 이동 외에 **그리퍼를 제어하는 네 번째 차원**을 추가했다.
    * **Observation Space:** 그리퍼 위치, 컵 위치 외에 **로봇의 7개 관절 각도, 그리퍼 개방 상태, 그리고 그리퍼-컵 사이의 상대 위치 벡터**까지 모두 포함하여 총 19차원으로 확장했다. 이를 통해 AI는 자신의 몸 상태와 목표와의 관계를 훨씬 더 풍부하게 인지할 수 있게 되었다.

3.  **물리 파라미터 튜닝 (마찰력):**
    * **문제:** 그리퍼가 컵을 잡아도 물리 시뮬레이션 상에서 미끄러져 놓칠 수 있다.
    * **해결:** `p.changeDynamics` 함수를 이용해 컵과 그리퍼의 **마찰력을 높여서** 잡기 성공률을 현실적으로 개선했다.

4.  **보상 함수 대폭 개선 ('보상의 계단'):**
    * AI가 막막한 최종 목표까지 쉽게 도달할 수 있도록, 다음과 같이 여러 단계의 '보상 계단'을 설계했다.
        1.  **접근 보상:** `(이전 거리 - 현재 거리)`에 비례한 보상.
        2.  **Z축 정렬 보상:** 그리퍼가 컵 위로 정렬되도록 유도.
        3.  **잡기 시도 보상:** 컵 근처에서 그리퍼를 닫는 '의도'에 대한 보상.
        4.  **잡기 성공 보상:** 그리퍼를 닫아 컵과 '접촉'한 상태에 대한 보상.
        5.  **들어 올리기 보상:** 컵을 잡고 들어 올리는 '높이'와 '속도'에 비례한 보상.
        6.  **최종 성공 보상:** 충분히 높이 들어 올렸을 때의 '잭팟'.

5.  **훈련 하이퍼파라미터 튜닝:**
    * 더 안정적이고 깊이 있는 학습을 위해 PPO 모델의 하이퍼파라미터를 조정했다.
    ```python
    model = PPO(
        "MlpPolicy", env, verbose=1,
        gamma=0.99,            # 미래 가치 중시
        learning_rate=0.0001,  # 안정적 학습
        n_steps=4096           # 풍부한 데이터 기반 업데이트
    )
    ```

---

## 4. 최종 성공 및 결론: AI를 '가르친다'는 것의 의미
위와 같은 복합적인 개선과 **100만 번 이상의 충분한 훈련** 끝에, AI는 마침내 **무작위 위치에 생성된 컵을 스스로 찾아 잡고 들어 올리는 데 성공**했다. 하지만 이 성공은 한 번에 오지 않았다. 오히려 수많은 실패와 그 원인을 분석하는 과정 속에서 얻어낸 값진 결과물이었다.
이번 프로젝트를 통해 강화학습의 성공은 단순히 좋은 알고리즘을 사용하는 것을 넘어, **AI의 눈높이에 맞춰 문제를 정의하고, 명확한 이정표를 제시하며, 점진적으로 난이도를 높여가는 체계적인 '교육 설계'가 핵심**임을 깨달았다.

깨달음 1: AI는 의도가 아닌 보상을 따른다.
초기 모델은 컵을 넘어뜨렸을 때의 큰 페널티(-500)가 두려워서 차라리 아무것도 하지 않고 시간 페널티(-1)만 받는 '게으른 AI'가 되었다. 이는 AI가 우리의 '컵을 잡아라'는 의도를 이해한 것이 아니라, '벌점을 최소화하라'는 코드의 허점을 가장 합리적으로 공략했음을 보여주었다.

깨달음 2: 중간 목표는 '디딤돌'이자 '함정'이 될 수 있다.
'게으른 AI' 문제를 해결하기 위해 '접촉 보상'과 '잡기 시도 보상'을 추가했을 때, AI는 적극적으로 컵에 접근하고 만지는 법을 배웠다. 하지만 이내 새로운 문제에 봉착했다. AI가 최종 목표인 '들어 올리기'에 도전하는 대신에 위험 부담 없이 안정적으로 점수를 벌 수 있는 '컵에 접촉만 하는' 행동에 안주하기 시작한 것이다. 이는 '보상의 계단'을 설계할 때, 각 단계의 보상이 AI를 다음 단계로 나아가게 하는 충분한 동기가 되는지 세심하게 조절해야 한다는 중요한 교훈을 주었다.

깨달음 3: '성공'의 정의는 명확하고 엄격해야 한다.
초기 성공 조건은 단순히 cup_pos[2] > 0.05 였다. 하지만 로봇팔이 컵을 툭 쳐서 살짝 튀어 올라도 '성공'으로 잘못 인식하는 허점을 발견했다. 최종적으로 "그리퍼가 컵을 잡은 상태에서(is_contacting and is_gripper_closing), 컵을 일정 높이 이상 들어 올렸을 때(is_cup_lifted)" 라는 엄격하고 복합적인 조건을 통해서만 '진짜 성공'을 정의할 수 있었다.

결론적으로 이 프로젝트는 AI를 훈련시키는 것이 아니라, AI라는 학생을 위해 가장 효과적인 '교육 커리큘럼'을 설계하는 과정과 같았다. AI의 이상 행동은 AI의 실수가 아닌, 나의 '교육 설계'에 대한 소중한 피드백이었다. 이 반복적인 디버깅과 개선의 과정이야말로 강화학습 개발의 진정한 핵심임을 직접 체험할 수 있었다.
