#  3D 공간 인지 파이프라인 학습 노트 

## 1. Why? - 문제 정의: 왜 2D를 넘어 3D로 가야 하는가?

이 프로젝트의 초기 목표는 YOLO와 같은 2D 객체 탐지 모델을 사용하는 것이었다. 하지만 이내 심각한 한계에 부딪혔다.

> **YOLO는 로봇의 '눈'이 될 수는 있지만, '팔'을 위한 길잡이가 될 수는 없다.**

YOLO는 카메라 이미지 속에서 컵이 어느 픽셀 `(u,v)`에 있는지, 얼마나 큰지(`w,h`)를 알려줄 뿐, 정작 로봇팔이 가장 궁금해하는 **"그래서 컵까지의 거리가 정확히 몇 미터인가?"** 에 대해서는 아무런 정보를 주지 못한다.

이 문제를 해결하고 로봇이 실제 물리 공간에서 작업을 수행하게 하려면 반드시 2D 픽셀 정보를 3D 공간 좌표로 변환하는 과정이 필요하다. 이 문서에서는 그 원리와 파이프라인을 정리한다.

---

## 2. What? - 핵심 개념 정리

### 2.1. 깊이(Depth) 정보란 무엇인가?

로봇이 3D 공간을 인식하기 위한 마지막 퍼즐 조각은 바로 '깊이' 정보다. 이 정보는 **Depth 카메라**라는 특별한 센서를 통해 얻을 수 있다.

뎁스 카메라는 두 종류의 결과물을 동시에 출력한다.

* **RGB 이미지:** 사람이 보는 것과 같은 일반적인 컬러 이미지. **"What"** 을 인식하는 데 사용된다. 
* **Depth Map:** 각 픽셀까지의 거리를 회색조로 표현한 이미지. 보통 밝을수록 가깝거나, 어두울수록 가깝게 설정된다. **"Where"** 를 알아내는 데 사용된다.



이 깊이 정보를 얻는 대표적인 기술에는 아래와 같은 것들이 있다.
* **스테레오 비전 (Stereo Vision):** 두 개의 카메라(사람의 두 눈처럼)를 이용해 시차를 계산하고, 삼각측량법으로 거리를 추정한다.
* **구조광 (Structured Light):** 특정 패턴의 적외선을 쏘고 물체 표면에서 패턴이 왜곡되는 정도를 분석하여 거리를 계산한다.
* **ToF (Time-of-Flight):** 빛을 쏘아 보낸 뒤 물체에 맞고 되돌아오는 시간을 직접 측정하여 거리를 계산한다.
* (자세한 내용: [e-con Systems 블로그](https://www.e-consystems.com/blog/camera/ko/technology-ko/what-are-depth-sensing-cameras-how-do-they-works/))

### 2.2. 카메라 캘리브레이션과 파라미터

2D 픽셀 좌표와 3D 공간 좌표를 변환하기 위해서는, 카메라의 고유한 '수학적 정보'가 필요하다. 이 정보는 **카메라 캘리브레이션(Camera Calibration)** 을 통해 얻을 수 있는데 크게 세 가지로 나뉜다.

1.  **내부 파라미터 (Intrinsic Parameters, K):** 카메라 렌즈와 센서 자체의 고유한 특성. (초점 거리 $f_x, f_y$ 와 주점 $c_x, c_y$ 등)
2.  **외부 파라미터 (Extrinsic Parameters, R, T):** 로봇이나 세상의 원점 대비 카메라가 **어디에 어떤 방향으로 놓여있는지**에 대한 정보. (회전 R, 평행이동 T)
3.  **왜곡 계수 (Distortion Coefficients):** 렌즈의 불완전함 때문에 발생하는 이미지 왜곡을 보정하기 위한 값.

> **Intrinsics는 '카메라의 스펙'이고, Extrinsics는 '카메라의 GPS와 나침반'이다.**

---

## 3. How? - 3D 인지 파이프라인의 작동 원리

로봇이 세상을 3D로 인식하는 과정은 아래와 같은 명확한 파이프라인을 따른다.

### ① [사전 준비] 카메라 캘리브레이션
* **언제:** 프로젝트 시작 시, 딱 한 번만 수행한다.
* **방법:** 체스보드 패턴을 여러 각도에서 촬영하여, 카메라의 **내부 파라미터(K)**와 **왜곡 계수(dist)**를 미리 계산해 둔다.
* **결과:** 2D-3D 변환과 왜곡 보정에 필요한 모든 값을 확보한다.

### ② [실시간] 데이터 획득 및 보정
* 뎁스 카메라로부터 **컬러 이미지**와 **뎁스 맵**을 동시에 입력받는다.
* 획득한 컬러 이미지에 왜곡 계수를 적용하여 **왜곡을 보정**한다. 

### ③ [실시간] 2D 객체 탐지
* 왜곡이 보정된 컬러 이미지를 YOLO 모델에 입력하여, 잡고 싶은 물체(컵)의 2D 픽셀 위치 `(u,v)`를 찾는다.

### ④ [실시간] 2D → 3D 변환 (Back Projection)
* 3단계에서 찾은 픽셀 위치 `(u,v)`를 이용해, **뎁스 맵**에서 해당 위치의 깊이 값 **Z**를 읽어온다.
* 아래의 **핵심 변환 공식**에 모든 값을 대입하여 **카메라 기준**의 3D 좌표 `(Xc, Yc, Zc)`를 계산한다.
    > $X_c = \dfrac{(u - c_x) \cdot Z}{f_x}$
    >
    > $Y_c = \dfrac{(v - c_y) \cdot Z}{f_y}$
    >
    > $Z_c = Z$

### ⑤ [실시간] 좌표계 변환
* ④에서 얻은 카메라 기준 3D 좌표 `(Xc, Yc, Zc)`에 **외부 파라미터(R, T)**를 적용하여, **로봇 기준**의 최종 3D 좌표 `(Xr, Yr, Zr)`로 변환한다.
    > $P_{robot} = R \cdot P_{camera} + T$

---

## 4. Aha! - 오늘 새롭게 깨달은 점

* 결과물을 실제 로봇이 사용 가능한 데이터로 '가공'하는 과정이 로보틱스에서는 중요하다는 것을 깨달았다.
* '카메라 캘리브레이션'이라는 작업이 왜 그렇게 중요한지, 그리고 그 결과물이 어떤 수학적 의미를 갖는지 명확하게 이해할 수 있었다.
* 포인트 클라우드가 결국 이미지와 깊이 정보로부터 계산된 결과물이라는 사실이 흥미로웠다.

## 5. To-Do - 추가적으로 궁금한 점

* 실제 카메라 렌즈는 왜곡(Distortion)이 있다고 하는데, 이 왜곡은 어떻게 보정할까? (카메라 캘리브레이션 시 왜곡 계수도 함께 계산된다고 함)
* 카메라 좌표계에서 계산된 (X,Y,Z)를 로봇팔의 베이스 좌표계로 변환하려면 어떻게 해야 할까? (아마 '외부 파라미터(Extrinsics)'와 관련 있을 것 같다.)
